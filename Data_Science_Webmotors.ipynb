{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python386jvsc74a57bd02db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37",
   "display_name": "Python 3.8.6 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "6e6cd1a37ae743b95addcaf84b2b82dd57fbb64481c9e7c7a479f657aeb68d7c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Este é meu primeiro projeto de Data Science com web crawling. Quero prever o preço de carros usando dados do site webmotors.\n",
    "Vou usar vários modelos e comparar os resultados deles ."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "O bloco a baixo faz o web scrapping do site WebMotors. Porém o site é dinâmico o que traz dificuldades. Usei o selenium para abrir o site e rolar até alcançar o final da página. Quando ele chega no final da página o beautifulsoup abre o código fonte, extraí os links e salva os links únicos (somente de carros) em um arquivo csv.\n",
    "Não recomendo rodar esse bloco, é bem demorado. No repositório já tem o arquivo csv com todos os links pegos."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import csv\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.webmotors.com.br/carros-usados/estoque')\n",
    "time.sleep(0.1)\n",
    "element = driver.find_element_by_xpath('/html/body/div/div[3]/div[2]/button')\n",
    "element.click()\n",
    "urls = open('urls2.csv', mode='w')\n",
    "url_writer = csv.writer(urls, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "heights = []\n",
    "links=[]\n",
    "\n",
    "\n",
    "for i in range(1,10000):\n",
    "    bg = driver.find_element_by_css_selector('body')\n",
    "    time.sleep(0.1)\n",
    "    bg.send_keys(Keys.END)\n",
    "    heights.append(driver.execute_script(\"return document.body.scrollHeight\"))\n",
    "    try :\n",
    "        bottom = heights[i-16]\n",
    "    except:\n",
    "        pass\n",
    "    if i%16 ==0:\n",
    "        new_bottom = heights[i-1]\n",
    "        if bottom == new_bottom:\n",
    "            try:\n",
    "                element = driver.find_element_by_xpath('/html/body/div/main/div[1]/div[3]/div[3]/div/div/button')\n",
    "                element.click()\n",
    "                soup = BeautifulSoup(driver.page_source, 'html')\n",
    "                for link in soup.findAll('a'):\n",
    "                    check_str = 'https://www.webmotors.com.br/comprar/'\n",
    "                    link_string = str(link.get('href'))\n",
    "                    res = check_str in link_string\n",
    "                    if link_string not in links and res == True:\n",
    "                        links.append(link_string)\n",
    "                        url_writer.writerow([link_string])\n",
    "\n",
    "            except:\n",
    "                break\n",
    "\n",
    "urls.close()"
   ]
  },
  {
   "source": [
    "Nessa próxima parte vamos tirar as caracteristícas de cada carro e organiza-las nas colunas 'modelo', 'combustível', 'ano','quilometragem', 'final de placa', 'carroceria' e 'cor'."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import csv\n",
    "import time\n",
    "from lxml import etree\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "urls = []\n",
    "with open('urls.csv') as f:\n",
    "    reader =csv.reader(f)\n",
    "    temp_urls = list(reader)\n",
    "\n",
    "for item in temp_urls:\n",
    "    urls.append(item[0])\n",
    "\n",
    "carros = open('carros.csv', mode='w')\n",
    "carros_writer = csv.writer(carros, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "carros_writer.writerow(['modelo' , 'combustivel' , 'ano' , 'quilometragem' , 'final de placa' , 'carroceria' , 'cor', 'ar condicionado', 'direcao', 'vidro eletrico', 'teto solar', 'alarme', 'rodas', 'retrovisores', 'computador de bordo'])\n",
    "\n",
    "chromeOptions = webdriver.ChromeOptions()\n",
    "chromeOptions.add_argument(\"--headless\")\n",
    "driver = webdriver.Chrome(options = chromeOptions)\n",
    "\n",
    "def parse(url):\n",
    "    driver.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html')\n",
    "    dom = etree.HTML(str(soup))\n",
    "\n",
    "    try:\n",
    "        \n",
    "        modelo = dom.xpath('/html/body/div/main/div[2]/div[1]/div/div[1]/div/div[1]/h1/strong')[0].text\n",
    "        combs = dom.xpath('/html/body/div/main/div[2]/div[1]/div/div[1]/div/div[3]/div/ul/li[5]/strong')[0].text\n",
    "        ano = dom.xpath('/html/body/div/main/div[2]/div[1]/div/div[1]/div/div[3]/div/ul/li[1]/strong')[0].text\n",
    "        km = dom.xpath('/html/body/div/main/div[2]/div[1]/div/div[1]/div/div[3]/div/ul/li[2]/strong')[0].text\n",
    "        placa = dom.xpath('/html/body/div/main/div[2]/div[1]/div/div[1]/div/div[3]/div/ul/li[6]/strong')[0].text\n",
    "        carro = dom.xpath('/html/body/div/main/div[2]/div[1]/div/div[1]/div/div[3]/div/ul/li[4]/strong')[0].text\n",
    "        cor = dom.xpath('/html/body/div/main/div[2]/div[1]/div/div[1]/div/div[3]/div/ul/li[7]/strong')[0].text\n",
    "        arcondicionado = 'nao'\n",
    "        direcao = ''\n",
    "        vidroeletrico = 'nao'\n",
    "        tetosolar = 'nao'\n",
    "        alarme = 'nao'\n",
    "        rodas = ''\n",
    "        retrovisores = ''\n",
    "        computador = 'nao'\n",
    "\n",
    "        for i in range(1,100):\n",
    "            try:\n",
    "\n",
    "                feature = str(dom.xpath(('/html/body/div/main/div[2]/div[1]/div/div[2]/div/ul/li[{0}]/h3').format(i))[0].text)\n",
    "    \n",
    "                if feature == 'Ar condicionado':\n",
    "                    arcondicionado = 'sim'\n",
    "\n",
    "                if feature == 'Direção hidráulica' or feature == 'Direção elétrica' or feature == 'Direção mecânica':\n",
    "                    direcao = feature\n",
    "\n",
    "                if feature == 'Vidros elétricos':\n",
    "                    vidroeletrico = 'sim'\n",
    "\n",
    "                if feature == 'Teto solar':\n",
    "                    tetosolar = 'sim'\n",
    "\n",
    "                if feature == 'Alarme':\n",
    "                    alarme = 'sim'\n",
    "                \n",
    "                if feature == 'Rodas de liga leve':\n",
    "                    rodas = feature\n",
    "\n",
    "                if feature == 'Retrovisores elétricos':\n",
    "                    retrovisores = feature\n",
    "\n",
    "                if feature == 'Computador de bordo':\n",
    "                    computador = feature\n",
    "                \n",
    "            except:\n",
    "                break\n",
    "        carros_writer.writerow( [ [modelo] , [combs] , [ano] , [km] , [placa] , [carro] , [cor], [arcondicionado], [direcao], [vidroeletrico], [tetosolar], [alarme], [rodas], [retrovisores], [computador] ] )\n",
    "        print([ [modelo] , [combs] , [ano] , [km] , [placa] , [carro] , [cor], [arcondicionado], [direcao], [vidroeletrico], [tetosolar], [alarme], [rodas], [retrovisores], [computador] ])\n",
    "  \n",
    "    except:\n",
    "\n",
    "        return\n",
    "def run_parallel_selenium_processes(datalist, selenium_func):\n",
    "\n",
    "    pool = Pool()\n",
    "\n",
    "    ITERATION_COUNT = cpu_count()-1\n",
    "\n",
    "    count_per_iteration = len(datalist) / float(ITERATION_COUNT)\n",
    "\n",
    "    for i in range(0, ITERATION_COUNT):\n",
    "        list_start = int(count_per_iteration * i)\n",
    "        list_end = int(count_per_iteration * (i+1))\n",
    "        pool.apply_async(selenium_func, [datalist[list_start:list_end]])\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "#run_parallel_selenium_processes(urls, parse)\n",
    "\n",
    "#pool = Pool()\n",
    "#pool.map(parse, urls)\n",
    "#pool.close()\n",
    "\n",
    "for url in urls:\n",
    "    parse(url)\n",
    "\n",
    "   "
   ]
  },
  {
   "source": [
    "Neste bloco vou analisar os dados adquiridos."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}