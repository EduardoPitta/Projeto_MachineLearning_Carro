{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python386jvsc74a57bd02db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37",
   "display_name": "Python 3.8.6 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "6e6cd1a37ae743b95addcaf84b2b82dd57fbb64481c9e7c7a479f657aeb68d7c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Este é meu primeiro projeto de Data Science com web crawling. Quero prever o preço de carros usando dados do site webmotors.\n",
    "Vou usar vários modelos e comparar os resultados deles ."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "O bloco a baixo faz o web scrapping do site WebMotors. Porém o site é dinâmico o que traz dificuldades. Usei o selenium para abrir o site e rolar até alcançar o final da página. Quando ele chega no final da página o beautifulsoup abre o código fonte, extraí os links e salva os links únicos (somente de carros) em um arquivo csv.\n",
    "Não recomendo rodar esse bloco, é bem demorado. No repositório já tem o arquivo csv com todos os links pegos."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import csv\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.webmotors.com.br/carros-usados/estoque')\n",
    "time.sleep(0.1)\n",
    "element = driver.find_element_by_xpath('/html/body/div/div[3]/div[2]/button')\n",
    "element.click()\n",
    "urls = open('urls2.csv', mode='w')\n",
    "url_writer = csv.writer(urls, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "heights = []\n",
    "links=[]\n",
    "counter = 0\n",
    "for i in range(1,10000):\n",
    "    bg = driver.find_element_by_css_selector('body')\n",
    "    time.sleep(0.1)\n",
    "    bg.send_keys(Keys.END)\n",
    "    heights.append(driver.execute_script(\"return document.body.scrollHeight\"))\n",
    "    try :\n",
    "        bottom = heights[i-16]\n",
    "    except:\n",
    "        pass\n",
    "    if i%16 ==0:\n",
    "        new_bottom = heights[i-1]\n",
    "        if bottom == new_bottom:\n",
    "            try:\n",
    "                element = driver.find_element_by_xpath('/html/body/div/main/div[1]/div[3]/div[3]/div/div/button')\n",
    "                element.click()\n",
    "                soup = BeautifulSoup(driver.page_source, 'html')\n",
    "                for link in soup.findAll('a'):\n",
    "                    check_str = 'https://www.webmotors.com.br/comprar/'\n",
    "                    link_string = str(link.get('href'))\n",
    "                    res = check_str in link_string\n",
    "                    if link_string not in links and res == True:\n",
    "                        links.append(link_string)\n",
    "                        url_writer.writerow([link_string])\n",
    "\n",
    "            except:\n",
    "                break\n",
    "\n",
    "urls.close()"
   ]
  },
  {
   "source": [
    "Nessa próxima parte vamos tirar as caracteristícas de cada carro e organiza-las nas colunas 'modelo', 'combustível', 'ano','quilometragem', 'final de placa', 'carroceria' e 'cor'."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "XPathEvalError",
     "evalue": "Invalid expression",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mXPathEvalError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-b67998840d7a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mcarro\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/html/body/div/main/div[2]/div[1]/div/div[1]/div/div[3]/div/ul/li[4]/strong'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mcor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/html/body/div/main/div[2]/div[1]/div/div[1]/div/div[3]/div/ul/li[7]/strong'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/html/body/div/main/div[2]/div[1]/div/div[2]/div/ul/li[3]/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[1;31m#/html/body/div/main/div[2]/div[1]/div/div[2]/div/ul/li[1]/h3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;31m#/html/body/div/main/div[2]/div[1]/div/div[2]/div/ul/li[2]/h3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32msrc\\lxml\\etree.pyx\u001b[0m in \u001b[0;36mlxml.etree._Element.xpath\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32msrc\\lxml\\xpath.pxi\u001b[0m in \u001b[0;36mlxml.etree.XPathElementEvaluator.__call__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32msrc\\lxml\\xpath.pxi\u001b[0m in \u001b[0;36mlxml.etree._XPathEvaluatorBase._handle_result\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mXPathEvalError\u001b[0m: Invalid expression"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import csv\n",
    "import time\n",
    "from lxml import etree\n",
    "\n",
    "urls = open('urls.csv', mode='r')\n",
    "carros = open('carros.csv', mode='w')\n",
    "carros_writer = csv.writer(carros, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "carros_writer.writerow(['modelo' , 'combustivel' , 'ano' , 'quilometragem' , 'final de placa' , 'carroceria' , 'cor'])\n",
    "for row in urls:\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(row,)\n",
    "    time.sleep(1)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html')\n",
    "    dom = etree.HTML(str(soup))\n",
    "\n",
    "    modelo = dom.xpath('/html/body/div/main/div[2]/div[1]/div/div[1]/div/div[1]/h1/strong')[0].text\n",
    "    combs = dom.xpath('/html/body/div/main/div[2]/div[1]/div/div[1]/div/div[3]/div/ul/li[5]/strong')[0].text\n",
    "    ano = dom.xpath('/html/body/div/main/div[2]/div[1]/div/div[1]/div/div[3]/div/ul/li[1]/strong')[0].text\n",
    "    km = dom.xpath('/html/body/div/main/div[2]/div[1]/div/div[1]/div/div[3]/div/ul/li[2]/strong')[0].text\n",
    "    placa = dom.xpath('/html/body/div/main/div[2]/div[1]/div/div[1]/div/div[3]/div/ul/li[6]/strong')[0].text\n",
    "    carro = dom.xpath('/html/body/div/main/div[2]/div[1]/div/div[1]/div/div[3]/div/ul/li[4]/strong')[0].text\n",
    "    cor = dom.xpath('/html/body/div/main/div[2]/div[1]/div/div[1]/div/div[3]/div/ul/li[7]/strong')[0].text\n",
    "    print(dom.xpath('/html/body/div/main/div[2]/div[1]/div/div[2]/div/ul/li[3]/h3')[0].text)\n",
    "\n",
    "    for i in range(1,100):\n",
    "        try: \n",
    "            feature = dom.xpath('/html/body/div/main/div[2]/div[1]/div/div[2]/div/ul/li[XXXXXXX]/h3')[0].text\n",
    "\n",
    "        except:\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    #/html/body/div/main/div[2]/div[1]/div/div[2]/div/ul/li[1]/h3\n",
    "    #/html/body/div/main/div[2]/div[1]/div/div[2]/div/ul/li[2]/h3\n",
    "   \n",
    "    for item in soup.findAll('h3'):\n",
    "        print(str(item.get('class')))\n",
    "        if str(item.get('href')) == 'Ar condicionado':\n",
    "            arcondicionado = 'sim'\n",
    "        else:\n",
    "            arcondicionado = 'nao'\n",
    "\n",
    "    carros_writer.writerow( [ [modelo] , [combs] , [ano] , [km] , [placa] , [carro] , [cor], [arcondicionado] ] )\n",
    "    print([modelo] , [combs] , [ano] , [km] , [placa] , [carro] , [cor], [arcondicionado])\n",
    "    driver.quit()\n",
    "\n"
   ]
  },
  {
   "source": [
    "Neste bloco vou analisar os dados adquiridos."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}